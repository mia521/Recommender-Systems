{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "def readCSV(path):\n",
    "  f = gzip.open(path, 'rt')\n",
    "  f.readline()\n",
    "  for l in f:\n",
    "    yield l.strip().split(',')\n",
    "    \n",
    "data = []\n",
    "for l in readCSV(\"train_Interactions.csv.gz\"):\n",
    "    data.append(l)\n",
    "train_data=data[:190000]\n",
    "valid_data=data[190000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Although we have built a validation set, it only consists of positive samples. For this task we also need examples of user/item pairs that weren’t read. For each entry (user,book) in the validation set, sample a negative entry by randomly choosing a book that user hasn’t read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_user=set(l[0] for l in data)\n",
    "unique_book=set(l[1] for l in data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "userbooks = defaultdict(set)\n",
    "bookusers = defaultdict(set)\n",
    "\n",
    "for user,book,r in readCSV(\"train_Interactions.csv.gz\"):\n",
    "  userbooks[user].add(book)\n",
    "  bookusers[book].add(user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "valid_read=[]\n",
    "n=0\n",
    "while n<10000:\n",
    "    for user, book, rating in valid_data:\n",
    "        read=userbooks[user]\n",
    "        unread=unique_book-read\n",
    "        valid_read.append((user, random.choice(list(unread)), 0))\n",
    "        n+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_read=[]\n",
    "for user, book, r in valid_data:\n",
    "    valid_read.append((user, book, 1))\n",
    "for user, book, r in train_data:\n",
    "    train_read.append((user, book, 1)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Evaluate the performance (accuracy) of the baseline model on the validation set you have built (1 mark)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bookCount = defaultdict(int)\n",
    "totalRead = 0\n",
    "\n",
    "for user,book,_ in readCSV(\"train_Interactions.csv.gz\"):\n",
    "  bookCount[book] += 1\n",
    "  totalRead += 1\n",
    "\n",
    "mostPopular = [(bookCount[x], x) for x in bookCount]\n",
    "mostPopular.sort()\n",
    "mostPopular.reverse()\n",
    "\n",
    "return1 = set()\n",
    "count = 0\n",
    "for ic, i in mostPopular:\n",
    "  count += ic\n",
    "  return1.add(i)\n",
    "  if count > totalRead/2: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_read_prediction=[]\n",
    "for u, b, _ in valid_read:\n",
    "    if b in return1:\n",
    "        valid_read_prediction.append((u, b, 1))\n",
    "    else:\n",
    "        valid_read_prediction.append((u, b, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6557"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct=0\n",
    "for i in range(0,len(valid_read_prediction)):\n",
    "    if valid_read_prediction[i][2] == valid_read[i][2]:\n",
    "        correct+=1\n",
    "accuracy = correct / len(valid_read)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. The existing ‘read prediction’ baseline just returns True if the item in question is ‘popular,’ using a threshold of the 50th percentile of popularity (totalRead/2). Assuming that the ‘non-read’ test examples are a random sample of user-book pairs, this threshold may not be the best one. See if you can find a better threshold and report its performance on your validation set (1 mark)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "thresholds=np.arange(0.01, 1, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies=[]\n",
    "def thresholdfinder(threshold):\n",
    "    return1 = set()\n",
    "    count = 0\n",
    "    for ic, i in mostPopular:\n",
    "      count += ic\n",
    "      return1.add(i)\n",
    "      if count > totalRead*threshold: break\n",
    "    valid_read_prediction=[]\n",
    "    for u, b, _ in valid_read:\n",
    "        if b in return1:\n",
    "            valid_read_prediction.append((u, b, 1))\n",
    "        else:\n",
    "            valid_read_prediction.append((u, b, 0))\n",
    "\n",
    "    correct=0\n",
    "    for i in range(0,len(valid_read_prediction)):\n",
    "        if valid_read_prediction[i][2] == valid_read[i][2]:\n",
    "            correct+=1\n",
    "\n",
    "    accuracy = correct / len(valid_read)\n",
    "    accuracies.append(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(thresholds)):\n",
    "     thresholdfinder(thresholds[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(list(zip(thresholds, accuracies)), columns =['threshold', 'accuracy'])\n",
    "import matplotlib.pyplot as plt \n",
    "df.plot(kind='line',x='threshold',y='accuracy')\n",
    "plt.show()\n",
    "threshold=thresholds[df['accuracy']==max(df['accuracy'])][0]\n",
    "df[df['accuracy']==max(df['accuracy'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('A threshold at the '+ str(int(threshold*100)) +'th percentile of popularity (TotalRead * ' + str(threshold) + ') achieves highest accuracy of ' + str(max(df['accuracy']))+' on the validation set.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. A stronger baseline than the one provided might make use of the Jaccard similarity (or another similarity metric). Given a pair (u, b) in the validation set, consider all training items b' that user u has read. For each, compute the Jaccard similarity between b and b', i.e., users (in the training set) who have read b and users who have read b'. Predict as ‘read’ if the maximum of these Jaccard similarities exceeds athreshold (you may choose the threshold that works best). Report the performance on your validation set (1 mark)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "userbooks_train = defaultdict(set)\n",
    "bookusers_train = defaultdict(set)\n",
    "\n",
    "for user,book,r in train_read:\n",
    "    userbooks_train[user].add(book)\n",
    "    bookusers_train[book].add(user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Jaccard(s1, s2):\n",
    "    numer = len(s1.intersection(s2))\n",
    "    denom = len(s1.union(s2))\n",
    "    return numer / denom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mostSimilar(user, book):\n",
    "    similarities = []\n",
    "    users = bookusers_train[book] # users who have read the query book\n",
    "    books = userbooks_train[user] # books the query user has read          \n",
    "    for readbook in books: # for every book the query user has read, find its user\n",
    "        similarusers=bookusers_train[readbook] # find similar users who has read one of the book read by the user\n",
    "        sim = Jaccard(users, similarusers)\n",
    "        similarities.append(sim)\n",
    "        similarities.sort(reverse=True)  \n",
    "    return similarities[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10344827586206896"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allsimilarities=[]\n",
    "for user,book,r in valid_read:\n",
    "    allsimilarities.append(mostSimilar(user, book))\n",
    "max(allsimilarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(allsimilarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_thresholdfinder(threshold):\n",
    "    jaccard_predictions=[]\n",
    "    for user,book,r in valid_read:\n",
    "        s = mostSimilar(user, book)\n",
    "        if s > threshold:\n",
    "            jaccard_predictions.append((user, book, 1))\n",
    "        else:\n",
    "            jaccard_predictions.append((user, book, 0))\n",
    "\n",
    "    correct=0\n",
    "    for i in range(0,len(valid_read_prediction)):\n",
    "        if jaccard_predictions[i][2] == valid_read[i][2]:\n",
    "                correct+=1\n",
    "    accuracy = correct / len(valid_read)\n",
    "    accuracies.append(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies=[]\n",
    "thresholds_jaccard=np.arange(0, 0.1, 0.002)\n",
    "for i in range(0,len(thresholds_jaccard)):\n",
    "     jaccard_thresholdfinder(thresholds_jaccard[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jaccard = pd.DataFrame(list(zip(thresholds_jaccard, accuracies)), columns =['threshold', 'accuracy'])\n",
    "df_jaccard[df_jaccard['accuracy']==max(df_jaccard['accuracy'])]\n",
    "df_jaccard.plot(kind='line',x='threshold',y='accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_jaccard=thresholds_jaccard[df_jaccard['accuracy']==max(df_jaccard['accuracy'])][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('A threshold at the '+ str(threshold_jaccard) +' achieves highest accuracy of ' + str(max(df_jaccard['accuracy']))+' on the validation set.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Improve the above predictor by incorporating both a Jaccard-based threshold and a popularity based threshold. Report the performance on your validation set (1 mark)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# two threshold finders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarities_valid=defaultdict(int)\n",
    "for u, b, _ in valid_read:\n",
    "    s = mostSimilar(u, b)\n",
    "    similarities_valid[u]+=(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies=[]\n",
    "def thresholdfinders(t1,t2):\n",
    "    prediction_4=[]\n",
    "    for u, b, _ in valid_read:\n",
    "        return1 = set()\n",
    "        count = 0\n",
    "        for ic, i in mostPopular:\n",
    "          count += ic\n",
    "          return1.add(i)\n",
    "          if count > totalRead*t1: break\n",
    "        if b in return1 and similarities_valid[u] > t2:\n",
    "            prediction_4.append((u, b, 1))\n",
    "        else:\n",
    "            prediction_4.append((u, b, 0))\n",
    "\n",
    "    correct=0\n",
    "    for i in range(0,len(prediction_4)):\n",
    "        if prediction_4[i][2] == valid_read[i][2]:\n",
    "                correct+=1\n",
    "    accuracy = correct / len(valid_read)\n",
    "    accuracies.append(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "boththreshold=[]\n",
    "t1=np.arange(0.5843, 0.5849, 0.0002)\n",
    "t2=np.arange(0, 0.003, 0.002)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I tried big ranges at first and then narrowed down this small range. To save running time, I only included this last range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies=[]\n",
    "for a in range(0,len(t1)):\n",
    "    for b in range(0,len(t2)):\n",
    "        thresholdfinders(t1[a],t2[b])\n",
    "        boththreshold.append((t1[a],t2[b]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_both = pd.DataFrame(list(zip(boththreshold, accuracies)), columns =['popularity & similarity threshold', 'accuracy'])\n",
    "df_both.plot(kind='line', y='accuracy')\n",
    "plt.show()\n",
    "df_both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "return1 = set()\n",
    "count = 0\n",
    "for ic, i in mostPopular:\n",
    "    count += ic\n",
    "    return1.add(i)\n",
    "    if count > totalRead*0.5847: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Cosine(s1, s2):\n",
    "    numer = len(s1.intersection(s2))   \n",
    "    denom = len(s1)*len(s2)\n",
    "    return numer / denom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mostCosine(user, book):\n",
    "    similarities = []\n",
    "    users = bookusers[book] # users who have read the query book\n",
    "    books = userbooks[user] # books the query user has read          \n",
    "    for readbook in books: # for every book the query user has read, find its user\n",
    "        if not readbook == book:\n",
    "            similarusers=bookusers[readbook] # find similar users who has read one of the book read by the user\n",
    "            sim = Cosine(users, similarusers)\n",
    "            similarities.append(sim)\n",
    "            similarities.sort(reverse=True)  \n",
    "        else:\n",
    "            similarities.append(0)\n",
    "    return similarities[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = open(\"predictions_Read.txt\", 'w')\n",
    "for l in open(\"pairs_Read.txt\"):\n",
    "    if l.startswith(\"userID\"):\n",
    "        #header\n",
    "        predictions.write(l)\n",
    "        continue\n",
    "    u,b = l.strip().split('-')\n",
    "    c = mostCosine(str(u), str(b))\n",
    "    if c > 0.0016:\n",
    "        predictions.write(u + '-' + b + \",1\\n\")\n",
    "    else:\n",
    "        predictions.write(u + '-' + b + \",0\\n\")\n",
    "predictions.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = open(\"predictions_Read.txt\", 'w')\n",
    "for l in open(\"pairs_Read.txt\"):\n",
    "    if l.startswith(\"userID\"):\n",
    "        #header\n",
    "        predictions.write(l)\n",
    "        continue\n",
    "    u,b = l.strip().split('-')\n",
    "    j = mostSimilar(str(u), str(b))\n",
    "    c = mostCosine(str(u), str(b))\n",
    "    if b in return1 and j > 0.002 and c > 0.0016:\n",
    "        predictions.write(u + '-' + b + \",1\\n\")\n",
    "    else:\n",
    "        predictions.write(u + '-' + b + \",0\\n\")\n",
    "predictions.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. To run our model on the test set, we’ll have to use the files ‘pairs Read.txt’ to find the reviewerID/itemID pairs about which we have to make predictions. Using that data, run the above model and upload your solution to Kaggle. Tell us your Kaggle user name (1 mark). If you’ve already uploaded a better solution to Kaggle, that’s fine too!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chengsheng Wu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. Fit a predictor of the form rating(user, item) = α + βuser + βitem, by fitting the mean and the two bias terms as described in the lecture notes. Use a regularization parameter of λ = 1. Report the MSE on the validation set (1 mark)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# calculate alpha, the global average\n",
    "allRatings = []\n",
    "for user,book,r in readCSV(\"train_Interactions.csv.gz\"):\n",
    "    r = int(r)\n",
    "    allRatings.append(r)\n",
    "globalAverage = sum(allRatings) / len(allRatings)\n",
    "alpha = globalAverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_user=list(set(l[0] for l in train_data))\n",
    "unique_item=list(set(l[1] for l in train_data))\n",
    "\n",
    "    \n",
    "user_map = {}\n",
    "for i, user in enumerate(unique_user):\n",
    "    user_map[user] = i\n",
    "item_map = {}\n",
    "for i, item in enumerate(unique_item):\n",
    "    item_map[item] = i\n",
    "\n",
    "rate_table = np.zeros((len(unique_user), len(unique_item)))\n",
    "for user,item,r in train_data:\n",
    "    rate_table[user_map[user], item_map[item]] = r\n",
    "\n",
    "user_items = defaultdict(list)\n",
    "item_users = defaultdict(list)\n",
    "for user, item, rating in train_data:\n",
    "    user_items[user].append(item)\n",
    "    item_users[item].append(user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_one(I, a1, b1, b2):\n",
    "    user,item = I[0],I[1]\n",
    "    if user in user_map:\n",
    "        if item in item_map:\n",
    "            return a1 + b1[user_map[user]] + b2[item_map[item]]\n",
    "    return alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "def train_parameter(a1, b1, b2, iters = 20, lambda_ = 1):\n",
    "    mse = 0.0\n",
    "    for iter_ in range(iters):\n",
    "        a1_ = 0.0\n",
    "        for I in train_data:\n",
    "            a1_ += (int(I[2]) - b1[user_map[I[0]]] - b2[item_map[I[1]]])\n",
    "        a1_ /= len(train_data)\n",
    "        a1 = a1_\n",
    "        for user_idx, user in enumerate(unique_user):\n",
    "            b1_ = 0.0\n",
    "            for item in user_items[user]:\n",
    "                item_id = item_map[item]\n",
    "                b1_ += (rate_table[user_idx,item_id] - b2[item_id] - a1)\n",
    "            b1_ /= (lambda_ + len(user_items[user]))\n",
    "            b1[user_idx] = b1_\n",
    "\n",
    "        for item_idx, item in enumerate(unique_item):\n",
    "            b2_ = 0.0\n",
    "            for user in item_users[item]:\n",
    "                user_id = user_map[user]\n",
    "                b2_ += (rate_table[user_id,item_idx] - b1[user_id] - a1)\n",
    "            b2_ /= (lambda_ + len(item_users[item]))\n",
    "            b2[item_idx] = b2_\n",
    "        mse = mean_squared_error([predict_one(I, a1, b1, b2) for I in valid_data], valid_ratings)\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_ratings=allRatings[190000:]\n",
    "b1 = np.random.rand(len(unique_user))\n",
    "b2 = np.random.rand(len(unique_item))\n",
    "a1 = alpha\n",
    "mse = train_parameter(a1, b1, b2, iters = 20, lambda_ = 1)\n",
    "print('Using Lambda:'+str(1)+' The MSR is: ' + str(mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10. Report the user and book IDs that have the largest and smallest values of β (1 mark)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_min = np.argmin(b1)\n",
    "index_max = np.argmax(b1)\n",
    "print(\"user with the smallest B is: \"+str(unique_user[index_min]))\n",
    "print(\"user with the largest B is: \"+str(unique_user[index_max]))\n",
    "index_min = np.argmin(b2)\n",
    "index_max = np.argmax(b2)\n",
    "print(\"item with the smallest B is: \"+str(unique_item[index_min]))\n",
    "print(\"item with the largest B is: \"+str(unique_item[index_max]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(unique_user)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11. Find a better value of λ using your validation set. Report the value you chose, its MSE, and upload your solution to Kaggle by running it on the test data (1 mark)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "las=np.arange(0.5, 4.5,0.5)\n",
    "b1 = np.random.rand(len(unique_user))\n",
    "b2 = np.random.rand(len(unique_item))\n",
    "for i in range(0,len(las)):\n",
    "    mse = train_parameter(a1, b1, b2, iters = 20, lambda_ = las[i])\n",
    "    print('Using Lambda:'+str(las[i])+' The MSR is: ' + str(mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Lambda=3. The MSR is: 1.1082729198454861"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_parameter(a1, b1, b2, iters = 100, lambda_ = 3):\n",
    "    mse = 0.0\n",
    "    for iter_ in range(iters):\n",
    "        a1_ = 0.0\n",
    "        for I in train_data:\n",
    "            a1_ += (int(I[2]) - b1[user_map[I[0]]] - b2[item_map[I[1]]])\n",
    "        a1_ /= len(train_data)\n",
    "        a1 = a1_\n",
    "        for user_idx, user in enumerate(unique_user):\n",
    "            b1_ = 0.0\n",
    "            for item in user_items[user]:\n",
    "                item_id = item_map[item]\n",
    "                b1_ += (rate_table[user_idx,item_id] - b2[item_id] - a1)\n",
    "            b1_ /= (lambda_ + len(user_items[user]))\n",
    "            b1[user_idx] = b1_\n",
    "\n",
    "        for item_idx, item in enumerate(unique_item):\n",
    "            b2_ = 0.0\n",
    "            for user in item_users[item]:\n",
    "                user_id = user_map[user]\n",
    "                b2_ += (rate_table[user_id,item_idx] - b1[user_id] - a1)\n",
    "            b2_ /= (lambda_ + len(item_users[item]))\n",
    "            b2[item_idx] = b2_\n",
    "        mse = mean_squared_error([predict_one(I, a1, b1, b2) for I in valid_data], valid_ratings)\n",
    "        print('At step' +str(iter_+1)+' The MSR is: ' + str(mse))\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b1 = np.random.rand(len(unique_user))\n",
    "b2 = np.random.rand(len(unique_item))\n",
    "train_parameter(a1, b1, b2, iters = 100, lambda_ = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_parameter(a1, b1, b2, iters = 100, lambda_ = 3):\n",
    "    mse = 0.0\n",
    "    for iter_ in range(iters):\n",
    "        a1_ = 0.0\n",
    "        for I in train_data:\n",
    "            a1_ += (int(I[2]) - b1[user_map[I[0]]] - b2[item_map[I[1]]])\n",
    "        a1_ /= len(train_data)\n",
    "        a1 = a1_\n",
    "        for user_idx, user in enumerate(unique_user):\n",
    "            b1_ = 0.0\n",
    "            for item in user_items[user]:\n",
    "                item_id = item_map[item]\n",
    "                b1_ += (rate_table[user_idx,item_id] - b2[item_id] - a1)\n",
    "            b1_ /= (lambda_ + len(user_items[user]))\n",
    "            b1[user_idx] = b1_\n",
    "\n",
    "        for item_idx, item in enumerate(unique_item):\n",
    "            b2_ = 0.0\n",
    "            for user in item_users[item]:\n",
    "                user_id = user_map[user]\n",
    "                b2_ += (rate_table[user_id,item_idx] - b1[user_id] - a1)\n",
    "            b2_ /= (lambda_ + len(item_users[item]))\n",
    "            b2[item_idx] = b2_\n",
    "        mse = mean_squared_error([predict_one(I, a1, b1, b2) for I in valid_data], valid_ratings)\n",
    "    return mse\n",
    "\n",
    "b1 = np.random.rand(len(unique_user))\n",
    "b2 = np.random.rand(len(unique_item))\n",
    "mse=train_parameter(a1, b1, b2, iters = 30, lambda_ = 3)\n",
    "print('Using Lambda 3 with '+str(30)+' iterations The MSR is: ' + str(mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_pair(user,item, a1, b1, b2): \n",
    "    if user in user_map:\n",
    "        if item in item_map:\n",
    "            return a1 + b1[user_map[user]] + b2[item_map[item]]\n",
    "    return alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = open(\"predictions_Rating.txt\", 'w')\n",
    "for l in open(\"pairs_Rating.txt\"):\n",
    "    if l.startswith(\"userID\"):\n",
    "        #header\n",
    "        predictions.write(l)\n",
    "    continue\n",
    "    user,item = l.strip().split('-')\n",
    "    rating = predict_pair(user,item, a1, b1, b2)\n",
    "    predictions.write(user + '-' + item + ',' + str(rating) + '\\n')\n",
    "predictions.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
